{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation\n",
        "\n",
        "Metrics from https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/context_precision/\n"
      ],
      "metadata": {
        "id": "EqHI8cXccuef"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definition:\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **Context Precision**\n",
        "- **Focus on _retrieved_ context**: how many of the retrieved context chunks are actually relevant to answering a question.\n",
        "- **Precision@k**: Measures how precise the context is at position _k_.\n",
        "- **Types**:\n",
        "  - **Without reference**  \n",
        "    - Compares retrieved context with **response**\n",
        "    - Compares each item in retrieved_contexts with the response using an LLM to determine how well the retrieved content supports the generated answer.\n",
        "\n",
        "  - **With reference**  \n",
        "    - Compares retrieved context with **reference** (gold answer)\n",
        "    - compare each retrieved_context with the reference — and determine how relevant or helpful that context is in supporting the reference answer\n",
        "- **Output**:\n",
        "  - `1.0`: Good — Retrieved context is highly relevant and supports the answer very well.\n",
        "  - `0.0`: Bad — Retrieved context is completely irrelevant to the answer.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Context Recall**\n",
        "- **Focus on _retrieved_ context**: How many parts of the gold answer (**reference**) can be found or supported in the retrieved context?\n",
        "- **Output**:\n",
        "  - High recall: Good — You retrieved most or all of the relevant documents.\n",
        "  - Low recall: Bad — You missed many relevant pieces.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Response Relevancy**\n",
        "- **Focus on _response_**: How relevant a generated response is to the original **user input** (the question).\n",
        "- **Output**:\n",
        "  - Higher score: Good — The response closely matches the intent and content of the user's question.\n",
        "  - Lower score: Bad — May indicate the response is off-topic, incomplete, or includes unnecessary info.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Faithfulness**\n",
        "- **Focus on _response_**: How factually accurate or consistent a response is with the **retrieved context**.\n",
        "- **Output**:\n",
        "  - `1.0`: Good — Fully faithful — all claims are supported by the context.\n",
        "  - `0.0`: Bad — Completely unfaithful — no claim can be verified from the context.\n"
      ],
      "metadata": {
        "id": "RNhZ_O1Fc0np"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation"
      ],
      "metadata": {
        "id": "B8hoVnrphERn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q ragas langchain openai"
      ],
      "metadata": {
        "id": "PUCqE5FOhUKQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b80f5158-62de-40e4-e2ff-d0bed4b10118"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/187.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m187.2/187.2 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/45.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m423.3/423.3 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "open_ai_key = getpass.getpass('Enter your OPENAI API Key')\n",
        "os.environ['OPENAI_API_KEY'] = open_ai_key"
      ],
      "metadata": {
        "id": "LXFhHIYAf_dy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e8df03b-5da8-41ba-f4fe-0a0ee4b4bc61"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your OPENAI API Key··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd  /content/drive/MyDrive/ECE1508_Project/Codes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CR-3w31MIK3J",
        "outputId": "7f4bd8ef-76ef-44e1-9252-74060b22e615"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/ECE1508_Project/Codes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional, List\n",
        "from ragas import SingleTurnSample\n",
        "from ragas.metrics import (\n",
        "    LLMContextPrecisionWithReference,\n",
        "    LLMContextRecall,\n",
        "    ResponseRelevancy,\n",
        "    Faithfulness\n",
        ")\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.embeddings import LangchainEmbeddingsWrapper"
      ],
      "metadata": {
        "id": "Yn5xpwUaqXX-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-3.5-turbo\"))\n",
        "evaluator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWgCWVD4qmbS",
        "outputId": "b020b176-b03f-48ba-b1d0-e5ef7335074e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-02884b6ee40f>:1: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
            "  evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-3.5-turbo\"))\n",
            "<ipython-input-5-02884b6ee40f>:2: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
            "  evaluator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the evaluation function\n",
        "async def evaluate_all_metrics(\n",
        "    user_input: Optional[str],\n",
        "    response: Optional[str],\n",
        "    reference: Optional[str],\n",
        "    retrieved_contexts: Optional[List[str]]\n",
        ") -> dict:\n",
        "\n",
        "    results = {\n",
        "        \"Context_Precision\": None,\n",
        "        \"Context_Recall\": None,\n",
        "        \"Response_Relevancy\": None,\n",
        "        \"Faithfulness\": None,\n",
        "    }\n",
        "\n",
        "    # Skip evaluation if required fields are missing\n",
        "    if not response or not retrieved_contexts:\n",
        "        return results\n",
        "\n",
        "    sample = SingleTurnSample(\n",
        "        user_input=user_input or \"\",\n",
        "        response=response,\n",
        "        reference=reference or \"\",\n",
        "        retrieved_contexts=retrieved_contexts\n",
        "    )\n",
        "\n",
        "    # Run metrics only if their required inputs are present\n",
        "    if retrieved_contexts and reference:\n",
        "        context_precision = LLMContextPrecisionWithReference(llm=evaluator_llm)\n",
        "        results[\"Context_Precision\"] = round(await context_precision.single_turn_ascore(sample),4)\n",
        "\n",
        "    if retrieved_contexts and reference:\n",
        "        context_recall = LLMContextRecall(llm=evaluator_llm)\n",
        "        results[\"Context_Recall\"] = round(await context_recall.single_turn_ascore(sample) ,4)\n",
        "\n",
        "    if user_input and response:\n",
        "        response_relevancy = ResponseRelevancy(llm=evaluator_llm, embeddings=evaluator_embeddings)\n",
        "        results[\"Response_Relevancy\"] = round(await response_relevancy.single_turn_ascore(sample),4)\n",
        "\n",
        "    if response and retrieved_contexts:\n",
        "        faithfulness = Faithfulness(llm=evaluator_llm)\n",
        "        results[\"Faithfulness\"] = round(await faithfulness.single_turn_ascore(sample),4)\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "cPFX7G41qTYh"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Single Test"
      ],
      "metadata": {
        "id": "AKL4xCijGXKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "result = await evaluate_all_metrics(\n",
        "    user_input=\"Where is the Eiffel Tower located?\",\n",
        "    response=\"The Eiffel Tower is located in Paris.\",\n",
        "    reference=\"The Eiffel Tower is located in Paris.\",\n",
        "    retrieved_contexts=[\"The Eiffel Tower is located in Paris.\"]\n",
        ")\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EK28SSDqbzm",
        "outputId": "55d62d86-a58a-41cc-e02a-e02a0136de62"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Context_Precision': 1.0, 'Context_Recall': 1.0, 'Response_Relevancy': np.float64(1.0), 'Faithfulness': 1.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Complete Evaluation"
      ],
      "metadata": {
        "id": "G5UMggudGY3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from datetime import datetime\n",
        "import time\n",
        "import tiktoken\n",
        "encoding = tiktoken.encoding_for_model(\"gpt-4\")"
      ],
      "metadata": {
        "id": "DGFRaNoDIBDY"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def evaluate_all(in_data):\n",
        "  for item in in_data:\n",
        "      user_input = item.get(\"input_question\")\n",
        "      response = item.get(\"response\")\n",
        "\n",
        "      # Combine long and short answers as reference\n",
        "      gold = item.get(\"gold_answer\", {})\n",
        "      long_answer = gold.get(\"long_answer\", \"\")\n",
        "      short_answers = gold.get(\"short_answers\", [])\n",
        "      combined_reference = long_answer + \" \" + \" \".join(short_answers)\n",
        "\n",
        "      retrieved_contexts = item.get(\"retrieved_contexts\")\n",
        "      evaluation = await evaluate_all_metrics(user_input, response, combined_reference, retrieved_contexts)\n",
        "      item[\"Evaluation\"] = evaluation\n",
        "  return in_data\n"
      ],
      "metadata": {
        "id": "QuIqFH3LGlJf"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load the file to be tested\n",
        "test_file_name = './evaluation/run_results_baseline.json'\n",
        "with open(test_file_name, \"r\", encoding=\"utf-8\") as f:\n",
        "    result_to_be_evaluated= json.load(f)"
      ],
      "metadata": {
        "id": "QxfFkXa3GWU3"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "eval_result = await evaluate_all(result_to_be_evaluated)\n",
        "end = time.time()\n",
        "\n",
        "print(f\"Evaluation of {test_file_name} took {end - start:.4f} seconds to run.\")"
      ],
      "metadata": {
        "id": "2MIRQJW-Hwac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc82e5f0-c589-4f2a-824f-d3b022c3521e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation of ./evaluation/run_results_baseline.json took 383.4580 seconds to run.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfacGqebJ-3M",
        "outputId": "04831c77-7be4-4076-ee14-e4af3b3b5fdd"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Baseline.ipynb\t\t 'L1_Process_Chunk&Save.ipynb'\t Proposition_Complete.ipynb\n",
            " Baseline_vector\t  L1_vector\t\t\t Proposition_Light.ipynb\n",
            " evaluation\t\t  L1_vector_test\t\t Proposition_Sample.ipynb\n",
            " Evaluation.ipynb\t  L1_vector_test_2\t\t rag_sw_ver2.ipynb\n",
            " gold_test_file_30.json   L2_vector_prop\t\t test_single_doc.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#save evaluation result\n",
        "today = datetime.today().strftime(\"%Y-%m-%d\")\n",
        "eval_result_file_name = f'./evaluation/eval_run_results_baseline_{today}.json'\n",
        "with open(eval_result_file_name, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(eval_result, f, indent=4, ensure_ascii=False)\n",
        "print(f\"Saved evaluated results to {eval_result_file_name}.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QC0hF7UeIWG5",
        "outputId": "0269f02c-fdbd-4956-9e47-ba598c676741"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved evaluated results to ./evaluation/eval_run_results_baseline_2025-04-05.json.json\n"
          ]
        }
      ]
    }
  ]
}