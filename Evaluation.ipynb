{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation\n",
        "\n",
        "Metrics from https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/context_precision/\n"
      ],
      "metadata": {
        "id": "EqHI8cXccuef"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definition:\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **Context Precision**\n",
        "- **Focus on _retrieved_ context**: how many of the retrieved context chunks are actually relevant to answering a question.\n",
        "- **Precision@k**: Measures how precise the context is at position _k_.\n",
        "- **Types**:\n",
        "  - **Without reference**  \n",
        "    - Compares retrieved context with **response**\n",
        "    - Compares each item in retrieved_contexts with the response using an LLM to determine how well the retrieved content supports the generated answer.\n",
        "\n",
        "  - **With reference**  \n",
        "    - Compares retrieved context with **reference** (gold answer)\n",
        "    - compare each retrieved_context with the reference — and determine how relevant or helpful that context is in supporting the reference answer\n",
        "- **Output**:\n",
        "  - `1.0`: Good — Retrieved context is highly relevant and supports the answer very well.\n",
        "  - `0.0`: Bad — Retrieved context is completely irrelevant to the answer.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Context Recall**\n",
        "- **Focus on _retrieved_ context**: How many parts of the gold answer (**reference**) can be found or supported in the retrieved context?\n",
        "- **Output**:\n",
        "  - High recall: Good — You retrieved most or all of the relevant documents.\n",
        "  - Low recall: Bad — You missed many relevant pieces.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Response Relevancy**\n",
        "- **Focus on _response_**: How relevant a generated response is to the original **user input** (the question).\n",
        "- **Output**:\n",
        "  - Higher score: Good — The response closely matches the intent and content of the user's question.\n",
        "  - Lower score: Bad — May indicate the response is off-topic, incomplete, or includes unnecessary info.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Faithfulness**\n",
        "- **Focus on _response_**: How factually accurate or consistent a response is with the **retrieved context**.\n",
        "- **Output**:\n",
        "  - `1.0`: Good — Fully faithful — all claims are supported by the context.\n",
        "  - `0.0`: Bad — Completely unfaithful — no claim can be verified from the context.\n"
      ],
      "metadata": {
        "id": "RNhZ_O1Fc0np"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation"
      ],
      "metadata": {
        "id": "B8hoVnrphERn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q ragas langchain openai"
      ],
      "metadata": {
        "id": "PUCqE5FOhUKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "open_ai_key = getpass.getpass('Enter your OPENAI API Key')\n",
        "os.environ['OPENAI_API_KEY'] = open_ai_key"
      ],
      "metadata": {
        "id": "LXFhHIYAf_dy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd  /content/drive/MyDrive/ECE1508_Project/Codes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CR-3w31MIK3J",
        "outputId": "7a172237-4a6d-43fd-93f0-665ec0869364"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/ECE1508_Project/Codes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional, List\n",
        "from ragas import SingleTurnSample\n",
        "from ragas.metrics import (\n",
        "    LLMContextPrecisionWithReference,\n",
        "    LLMContextRecall,\n",
        "    ResponseRelevancy,\n",
        "    Faithfulness\n",
        ")\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.embeddings import LangchainEmbeddingsWrapper"
      ],
      "metadata": {
        "id": "Yn5xpwUaqXX-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-3.5-turbo\"))\n",
        "evaluator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWgCWVD4qmbS",
        "outputId": "079e7219-4a5a-4c40-a217-09eae5a51d2e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-02884b6ee40f>:1: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
            "  evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-3.5-turbo\"))\n",
            "<ipython-input-5-02884b6ee40f>:2: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
            "  evaluator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the evaluation function\n",
        "async def evaluate_all_metrics(\n",
        "    user_input: Optional[str],\n",
        "    response: Optional[str],\n",
        "    reference: Optional[str],\n",
        "    retrieved_contexts: Optional[List[str]]\n",
        ") -> dict:\n",
        "\n",
        "    results = {\n",
        "        \"Context_Precision\": None,\n",
        "        \"Context_Recall\": None,\n",
        "        \"Response_Relevancy\": None,\n",
        "        \"Faithfulness\": None,\n",
        "    }\n",
        "\n",
        "    # Skip evaluation if required fields are missing\n",
        "    if not response or not retrieved_contexts:\n",
        "        return results\n",
        "\n",
        "    sample = SingleTurnSample(\n",
        "        user_input=user_input or \"\",\n",
        "        response=response,\n",
        "        reference=reference or \"\",\n",
        "        retrieved_contexts=retrieved_contexts\n",
        "    )\n",
        "\n",
        "    # Run metrics only if their required inputs are present\n",
        "    if retrieved_contexts and reference:\n",
        "        context_precision = LLMContextPrecisionWithReference(llm=evaluator_llm)\n",
        "        results[\"Context_Precision\"] = round(await context_precision.single_turn_ascore(sample),4)\n",
        "\n",
        "    if retrieved_contexts and reference:\n",
        "        context_recall = LLMContextRecall(llm=evaluator_llm)\n",
        "        results[\"Context_Recall\"] = round(await context_recall.single_turn_ascore(sample) ,4)\n",
        "\n",
        "    if user_input and response:\n",
        "        response_relevancy = ResponseRelevancy(llm=evaluator_llm, embeddings=evaluator_embeddings)\n",
        "        results[\"Response_Relevancy\"] = round(await response_relevancy.single_turn_ascore(sample),4)\n",
        "\n",
        "    if response and retrieved_contexts:\n",
        "        faithfulness = Faithfulness(llm=evaluator_llm)\n",
        "        results[\"Faithfulness\"] = round(await faithfulness.single_turn_ascore(sample),4)\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "cPFX7G41qTYh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Single Test"
      ],
      "metadata": {
        "id": "AKL4xCijGXKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "result = await evaluate_all_metrics(\n",
        "    user_input=\"Where is the Eiffel Tower located?\",\n",
        "    response=\"The Eiffel Tower is located in Paris.\",\n",
        "    reference=\"The Eiffel Tower is located in Paris.\",\n",
        "    retrieved_contexts=[\"The Eiffel Tower is located in Paris.\"]\n",
        ")\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EK28SSDqbzm",
        "outputId": "7c337bfa-7ee6-4dce-dcf1-cc20fe871744"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Context_Precision': 1.0, 'Context_Recall': 1.0, 'Response_Relevancy': np.float64(1.0), 'Faithfulness': 1.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Complete Evaluation"
      ],
      "metadata": {
        "id": "G5UMggudGY3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "DGFRaNoDIBDY"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def evaluate_all(in_data):\n",
        "  for item in in_data:\n",
        "      user_input = item.get(\"input_question\")\n",
        "      response = item.get(\"response\")\n",
        "      reference = item.get(\"gold_answer\", {}).get(\"long_answer\")\n",
        "      retrieved_contexts = item.get(\"retrieved_contexts\")\n",
        "      evaluation = await evaluate_all_metrics(user_input, response, reference, retrieved_contexts)\n",
        "      item[\"Evaluation\"] = evaluation\n",
        "  return in_data\n"
      ],
      "metadata": {
        "id": "QuIqFH3LGlJf"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load the file to be tested\n",
        "test_file_name = './evaluation/run_results_proposition.json'\n",
        "with open(test_file_name, \"r\", encoding=\"utf-8\") as f:\n",
        "    result_to_be_evaluated= json.load(f)"
      ],
      "metadata": {
        "id": "QxfFkXa3GWU3"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_result = await evaluate_all(result_to_be_evaluated)"
      ],
      "metadata": {
        "id": "2MIRQJW-Hwac"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfacGqebJ-3M",
        "outputId": "a84ac692-1fc5-433e-f0c5-56403e20b116"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " evaluation\t\t        L1_vector_test\t\t     Proposition_Sample.ipynb\n",
            " Evaluation.ipynb\t        L1_vector_test_2\t     rag_sw_ver2.ipynb\n",
            " gold_test_file_30.json         L2_vector_prop\t\t     run_results_proposition.json\n",
            "'L1_Process_Chunk&Save.ipynb'   Proposition_Complete.ipynb   test_single_doc.json\n",
            " L1_vector\t\t        Proposition_Light.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#save evaluation result\n",
        "today = datetime.today().strftime(\"%Y-%m-%d\")\n",
        "eval_result_file_name = f'./evaluation/eval_run_results_proposition_{today}.json'\n",
        "with open(eval_result_file_name, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(eval_result, f, indent=4, ensure_ascii=False)\n",
        "print(f\"Saved evaluated results to {eval_result_file_name}.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QC0hF7UeIWG5",
        "outputId": "d7e6061b-eb8b-43a3-ea77-47ae2c922741"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved evaluated results to ./evaluation/eval_run_results_proposition_2025-04-04.json.json\n"
          ]
        }
      ]
    }
  ]
}