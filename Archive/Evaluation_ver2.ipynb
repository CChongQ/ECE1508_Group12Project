{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation\n",
        "\n",
        "Metrics from https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/context_precision/\n"
      ],
      "metadata": {
        "id": "EqHI8cXccuef"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definition:\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **Context Precision**\n",
        "- **Focus on _retrieved_ context**: how many of the retrieved context chunks are actually relevant to answering a question.\n",
        "- **Precision@k**: Measures how precise the context is at position _k_.\n",
        "- **Types**:\n",
        "  - **Without reference**  \n",
        "    - Compares retrieved context with **response**\n",
        "    - Compares each item in retrieved_contexts with the response using an LLM to determine how well the retrieved content supports the generated answer.\n",
        "\n",
        "  - **With reference**  \n",
        "    - Compares retrieved context with **reference** (gold answer)\n",
        "    - compare each retrieved_context with the reference — and determine how relevant or helpful that context is in supporting the reference answer\n",
        "- **Output**:\n",
        "  - `1.0`: Good — Retrieved context is highly relevant and supports the answer very well.\n",
        "  - `0.0`: Bad — Retrieved context is completely irrelevant to the answer.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Context Recall**\n",
        "- **Focus on _retrieved_ context**: How many parts of the gold answer (**reference**) can be found or supported in the retrieved context?\n",
        "- **Output**:\n",
        "  - High recall: Good — You retrieved most or all of the relevant documents.\n",
        "  - Low recall: Bad — You missed many relevant pieces.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Response Relevancy**\n",
        "- **Focus on _response_**: How relevant a generated response is to the original **user input** (the question).\n",
        "- **Output**:\n",
        "  - Higher score: Good — The response closely matches the intent and content of the user's question.\n",
        "  - Lower score: Bad — May indicate the response is off-topic, incomplete, or includes unnecessary info.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Faithfulness**\n",
        "- **Focus on _response_**: How factually accurate or consistent a response is with the **retrieved context**.\n",
        "- **Output**:\n",
        "  - `1.0`: Good — Fully faithful — all claims are supported by the context.\n",
        "  - `0.0`: Bad — Completely unfaithful — no claim can be verified from the context.\n"
      ],
      "metadata": {
        "id": "RNhZ_O1Fc0np"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation"
      ],
      "metadata": {
        "id": "B8hoVnrphERn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q ragas langchain openai"
      ],
      "metadata": {
        "id": "PUCqE5FOhUKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "open_ai_key = getpass.getpass('Enter your OPENAI API Key')\n",
        "os.environ['OPENAI_API_KEY'] = open_ai_key"
      ],
      "metadata": {
        "id": "LXFhHIYAf_dy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e8df03b-5da8-41ba-f4fe-0a0ee4b4bc61"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your OPENAI API Key··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd  /content/drive/MyDrive/ECE1508_Project/Codes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CR-3w31MIK3J",
        "outputId": "7f4bd8ef-76ef-44e1-9252-74060b22e615"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/ECE1508_Project/Codes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional, List\n",
        "from ragas import SingleTurnSample\n",
        "from ragas.metrics import (\n",
        "    LLMContextPrecisionWithReference,\n",
        "    LLMContextRecall,\n",
        "    ResponseRelevancy,\n",
        "    Faithfulness\n",
        ")\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.embeddings import LangchainEmbeddingsWrapper"
      ],
      "metadata": {
        "id": "Yn5xpwUaqXX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-3.5-turbo\"))\n",
        "evaluator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())"
      ],
      "metadata": {
        "id": "kWgCWVD4qmbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the evaluation function\n",
        "async def evaluate_all_metrics(\n",
        "    user_input: Optional[str],\n",
        "    response: Optional[str],\n",
        "    reference: Optional[str],\n",
        "    retrieved_contexts: Optional[List[str]]\n",
        ") -> dict:\n",
        "\n",
        "    results = {\n",
        "        \"Context_Precision\": None,\n",
        "        \"Context_Recall\": None,\n",
        "        \"Response_Relevancy\": None,\n",
        "        \"Faithfulness\": None,\n",
        "    }\n",
        "\n",
        "    # Skip evaluation if required fields are missing\n",
        "    if not response or not retrieved_contexts:\n",
        "        return results\n",
        "\n",
        "    sample = SingleTurnSample(\n",
        "        user_input=user_input or \"\",\n",
        "        response=response,\n",
        "        reference=reference or \"\",\n",
        "        retrieved_contexts=retrieved_contexts\n",
        "    )\n",
        "\n",
        "    # Run metrics only if their required inputs are present\n",
        "    if retrieved_contexts and reference:\n",
        "        context_precision = LLMContextPrecisionWithReference(llm=evaluator_llm)\n",
        "        results[\"Context_Precision\"] = round(await context_precision.single_turn_ascore(sample),4)\n",
        "\n",
        "    if retrieved_contexts and reference:\n",
        "        context_recall = LLMContextRecall(llm=evaluator_llm)\n",
        "        results[\"Context_Recall\"] = round(await context_recall.single_turn_ascore(sample) ,4)\n",
        "\n",
        "    if user_input and response:\n",
        "        response_relevancy = ResponseRelevancy(llm=evaluator_llm, embeddings=evaluator_embeddings)\n",
        "        results[\"Response_Relevancy\"] = round(await response_relevancy.single_turn_ascore(sample),4)\n",
        "\n",
        "    if response and retrieved_contexts:\n",
        "        faithfulness = Faithfulness(llm=evaluator_llm)\n",
        "        results[\"Faithfulness\"] = round(await faithfulness.single_turn_ascore(sample),4)\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "cPFX7G41qTYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Single Test"
      ],
      "metadata": {
        "id": "AKL4xCijGXKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "result = await evaluate_all_metrics(\n",
        "    user_input=\"Where is the Eiffel Tower located?\",\n",
        "    response=\"The Eiffel Tower is located in Paris.\",\n",
        "    reference=\"The Eiffel Tower is located in Paris.\",\n",
        "    retrieved_contexts=[\"The Eiffel Tower is located in Paris.\"]\n",
        ")\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EK28SSDqbzm",
        "outputId": "8e651fd4-e30a-4469-8894-27d9c4b4ac4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Context_Precision': 1.0, 'Context_Recall': 1.0, 'Response_Relevancy': np.float64(1.0), 'Faithfulness': 1.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Complete Evaluation"
      ],
      "metadata": {
        "id": "G5UMggudGY3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from datetime import datetime\n",
        "import time\n",
        "import tiktoken\n",
        "encoding = tiktoken.encoding_for_model(\"gpt-4\")"
      ],
      "metadata": {
        "id": "DGFRaNoDIBDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def evaluate_all(in_data):\n",
        "  for item in in_data:\n",
        "      user_input = item.get(\"input_question\")\n",
        "\n",
        "      response = item.get(\"response\")\n",
        "\n",
        "      # Combine long and short answers as reference\n",
        "      gold = item.get(\"gold_answer\", {})\n",
        "      long_answer = gold.get(\"long_answer\", \"\")\n",
        "      short_answers = gold.get(\"short_answers\", [])\n",
        "      combined_reference = long_answer + \" \" + \" \".join(short_answers)\n",
        "\n",
        "      retrieved_contexts = item.get(\"retrieved_contexts\")\n",
        "      evaluation = await evaluate_all_metrics(user_input, response, combined_reference, retrieved_contexts)\n",
        "      item[\"Evaluation\"] = evaluation\n",
        "  return in_data\n"
      ],
      "metadata": {
        "id": "QuIqFH3LGlJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load the file to be tested\n",
        "test_file_name = './evaluation/run_results_baseline.json'\n",
        "with open(test_file_name, \"r\", encoding=\"utf-8\") as f:\n",
        "    result_to_be_evaluated= json.load(f)"
      ],
      "metadata": {
        "id": "QxfFkXa3GWU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "eval_result = await evaluate_all(result_to_be_evaluated)\n",
        "end = time.time()\n",
        "\n",
        "print(f\"Evaluation of {test_file_name} took {end - start:.4f} seconds to run.\")"
      ],
      "metadata": {
        "id": "2MIRQJW-Hwac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ef0b3d8-6acd-43ec-b6ac-4507bcff6f2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation of ./evaluation/run_results_baseline.json took 488.7995 seconds to run.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfacGqebJ-3M",
        "outputId": "f475ae71-b84f-4d4c-c469-f0d941382ac4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Baseline.ipynb\t\t        L1_vector\t\t     Proposition_Sample.ipynb\n",
            " Baseline_vector\t        L1_vector_test\t\t     rag_sw_ver2.ipynb\n",
            " evaluation\t\t        L1_vector_test_2\t     rag_sw_ver3.ipynb\n",
            " Evaluation.ipynb\t        L2_vector_prop\t\t     test_single_doc.json\n",
            " gold_test_file_30.json         Proposition_Complete.ipynb\n",
            "'L1_Process_Chunk&Save.ipynb'   Proposition_Light.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#save evaluation result\n",
        "today = datetime.today().strftime(\"%Y-%m-%d\")\n",
        "eval_result_file_name = f'./evaluation/eval_run_results_baseline_{today}.json'\n",
        "with open(eval_result_file_name, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(eval_result, f, indent=4, ensure_ascii=False)\n",
        "print(f\"Saved evaluated results to {eval_result_file_name}.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QC0hF7UeIWG5",
        "outputId": "7c3855f4-775f-4bd5-b0f5-c7a27e753808"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved evaluated results to ./evaluation/eval_run_results_baseline_2025-04-05.json.json\n"
          ]
        }
      ]
    }
  ]
}