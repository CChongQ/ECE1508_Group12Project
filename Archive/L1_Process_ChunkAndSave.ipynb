{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Environmetn Setup"
      ],
      "metadata": {
        "id": "JGEI9OjIrbA2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCc9CdAkexQA"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet --upgrade langchain-text-splitters langchain-community langgraph\n",
        "!pip install -qU \"langchain[openai]\" # select chat model OpenAI\n",
        "!pip install -U langchain langchain-core langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-openai # select embeddings model OpenAI\n",
        "!pip install -qU langchain-community # select vector store FAISS\n",
        "!pip install jq\n",
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "id": "EIJKBWd_fQrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "import json\n",
        "import faiss\n",
        "import re\n",
        "import getpass\n",
        "import os"
      ],
      "metadata": {
        "id": "Qy3MpCwZFR8L"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain_community.document_loaders import JSONLoader\n",
        "from langchain_text_splitters import HTMLSectionSplitter\n",
        "from langchain_core.documents import Document\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.output_parsers.openai_tools import JsonOutputToolsParser\n",
        "from langchain_community.chat_models import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from langchain.chains import create_extraction_chain\n",
        "from langchain.chains import create_extraction_chain_pydantic\n",
        "from langchain import hub\n",
        "from langchain_core.pydantic_v1 import BaseModel\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "from pydantic import BaseModel\n",
        "from typing import Optional, List\n"
      ],
      "metadata": {
        "id": "ClKCfdTSGZgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
        "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")"
      ],
      "metadata": {
        "id": "YS1gRamHfY1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90c2faeb-428a-41dc-f30e-fd4cc19cb369"
      },
      "execution_count": 5,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter API key for OpenAI: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd  /content/drive/MyDrive/ECE1508_Project/Codes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKEpu0O6fgMx",
        "outputId": "447394fe-fab5-4c52-9268-34c44663b8ce"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/ECE1508_Project/Codes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helpers"
      ],
      "metadata": {
        "id": "s5eNbOcwD6o9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Test Doc"
      ],
      "metadata": {
        "id": "MuhooPOlrhK1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def metadata_func(example: dict, _: dict) -> dict:\n",
        "    return {\n",
        "        \"document_url\": example.get(\"document_url\"),\n",
        "        \"question_text\": example.get(\"question_text\"),\n",
        "        \"annotations\": example.get(\"annotations\"),\n",
        "        \"example_id\": example.get(\"example_id\"),\n",
        "        \"gold_answer\": example.get(\"gold_answer\"),\n",
        "        \"Title\": example.get(\"title\", \"Untitled\")\n",
        "    }"
      ],
      "metadata": {
        "id": "cFP1-rLTgr11"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_documents(file_path):\n",
        "  loader=JSONLoader(\n",
        "    file_path=file_path,\n",
        "    jq_schema=\".[]\",\n",
        "    content_key=\"document_text\",\n",
        "    metadata_func=metadata_func\n",
        "  )\n",
        "  documents=loader.load()\n",
        "\n",
        "  return documents"
      ],
      "metadata": {
        "id": "AKoIS4oc9LFJ"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vector Store Helpers"
      ],
      "metadata": {
        "id": "zIcK9WKO48gk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_faiss_vec_store(elemnts_to_emb, folder_name):\n",
        "  vectorstore=FAISS.from_documents(elemnts_to_emb,embedding=OpenAIEmbeddings())\n",
        "  vectorstore.save_local(folder_name)\n",
        "  return vectorstore"
      ],
      "metadata": {
        "id": "VkWWcJwu4_mh"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retriver Helpers"
      ],
      "metadata": {
        "id": "o7-Cx6fN6Off"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# retrieve the top K contents\n",
        "def retrieve_section(in_retriever,query,top_k):\n",
        "  results=in_retriever.get_relevant_documents(query)\n",
        "  if not results:\n",
        "    return None\n",
        "  top_5_match=results[:top_k]\n",
        "  # for i in range(5):\n",
        "  #   print(f\"No.{i+1} chunk: {top_5_match[i]}\")\n",
        "\n",
        "  return top_5_match\n"
      ],
      "metadata": {
        "id": "b5gC4gab6QCC"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Run retriver for the input query\n",
        "def get_retrieve_section(in_retriever,in_query,top_k):\n",
        "  print(f\"Retrieving answer for query: {in_query}\")\n",
        "  relevant_sections=retrieve_section(in_retriever,in_query,top_k)\n",
        "  return relevant_sections"
      ],
      "metadata": {
        "id": "z-N6Hacn6SPI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Level 1 Helpers"
      ],
      "metadata": {
        "id": "XrOFCx9fqrn4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Document Chunking**"
      ],
      "metadata": {
        "id": "kiNotLnAurs6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_element_chunk(split_header_list,doc_to_chunk):\n",
        "\n",
        "  #Wrap the the original HTML content in a temporary Document object\n",
        "  html_doc = Document(page_content=doc_to_chunk.page_content, metadata=doc_to_chunk.metadata)\n",
        "\n",
        "\n",
        "  #Only split the HTML part\n",
        "  html_splitter = HTMLSectionSplitter(headers_to_split_on=split_header_list)\n",
        "  elements_chunked = html_splitter.split_documents([html_doc])\n",
        "  return elements_chunked\n"
      ],
      "metadata": {
        "id": "927QAUBXtetL"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Level 1 Chunking & Save to Vector\n"
      ],
      "metadata": {
        "id": "E8JCYikZ7Vi9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Vector stpre folder path\n",
        "#Recheck pwd\n",
        "!ls\n",
        "L1_vector_folder = 'L1_vector_test'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nI2GBr_cECn7",
        "outputId": "e5ec9575-a7cc-4eaf-c621-521f23ef80ca"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gold_test_file_30.json\tL1_vector_test\t\t    Proposition_Sample.ipynb\n",
            "L1_Processing.ipynb\tL2_vector_prop\t\t    rag_sw_ver2.ipynb\n",
            "L1_vector\t\tProposition_Complete.ipynb  test_single_doc.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path=\"gold_test_file_30.json\"\n",
        "test_documents = load_documents(file_path)\n",
        "print(f\"{len(test_documents)} Documents\")"
      ],
      "metadata": {
        "id": "HwvSRP-o8zCr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aad0d9a8-861c-4f91-8f20-bfdaf5c7b79c"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30 Documents\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: L1 basic chunking\n",
        "\n",
        "Chunk all documents in the input json and save into vector database"
      ],
      "metadata": {
        "id": "Dq01zDhb_5e7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def L1_process_document(doc):\n",
        "  headers_to_split_on = [\n",
        "    (\"h1\", \"Header 1\"),\n",
        "    (\"h2\", \"Header 2\"),\n",
        "    (\"h3\", \"Header 3\"),\n",
        "  ]\n",
        "\n",
        "  all_chunks = []\n",
        "  for idx,eachDoc in enumerate(doc):\n",
        "      chunks = get_element_chunk(headers_to_split_on, eachDoc)\n",
        "      all_chunks.extend(chunks)\n",
        "      print(f\"Split document {idx+1} into {len(chunks)} sub-documents.\")\n",
        "      #print(f\"Example 1: {chunks[1]}\")\n",
        "\n",
        "  #Embed and Vector store\n",
        "  L1_vectorstore=create_faiss_vec_store(all_chunks,L1_vector_folder)\n",
        "\n",
        "  print(f\"{len(doc)} documents sucessfully processed and saved to {L1_vector_folder}\")\n",
        "\n",
        "  return L1_vectorstore\n",
        "\n"
      ],
      "metadata": {
        "id": "Mjd2YDe39G83"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "L1_vectorstore = L1_process_document(test_documents)"
      ],
      "metadata": {
        "id": "o8iy3A38AYi7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81842e48-2afe-4e52-8e41-04d935a4718a"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split document 1 into 14 sub-documents.\n",
            "Split document 2 into 13 sub-documents.\n",
            "Split document 3 into 18 sub-documents.\n",
            "Split document 4 into 21 sub-documents.\n",
            "Split document 5 into 14 sub-documents.\n",
            "Split document 6 into 20 sub-documents.\n",
            "Split document 7 into 26 sub-documents.\n",
            "Split document 8 into 12 sub-documents.\n",
            "Split document 9 into 25 sub-documents.\n",
            "Split document 10 into 19 sub-documents.\n",
            "Split document 11 into 42 sub-documents.\n",
            "Split document 12 into 16 sub-documents.\n",
            "Split document 13 into 8 sub-documents.\n",
            "Split document 14 into 29 sub-documents.\n",
            "Split document 15 into 43 sub-documents.\n",
            "Split document 16 into 37 sub-documents.\n",
            "Split document 17 into 27 sub-documents.\n",
            "Split document 18 into 78 sub-documents.\n",
            "Split document 19 into 18 sub-documents.\n",
            "Split document 20 into 18 sub-documents.\n",
            "Split document 21 into 15 sub-documents.\n",
            "Split document 22 into 29 sub-documents.\n",
            "Split document 23 into 37 sub-documents.\n",
            "Split document 24 into 58 sub-documents.\n",
            "Split document 25 into 12 sub-documents.\n",
            "Split document 26 into 16 sub-documents.\n",
            "Split document 27 into 7 sub-documents.\n",
            "Split document 28 into 13 sub-documents.\n",
            "Split document 29 into 48 sub-documents.\n",
            "Split document 30 into 15 sub-documents.\n",
            "30 documents sucessfully processed and saved to L1_vector_test\n"
          ]
        }
      ]
    }
  ]
}