{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "MuhooPOlrhK1",
        "zIcK9WKO48gk",
        "o7-Cx6fN6Off",
        "XrOFCx9fqrn4",
        "-71C2WjY2dsk"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Environmetn Setup"
      ],
      "metadata": {
        "id": "JGEI9OjIrbA2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCc9CdAkexQA"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet --upgrade langchain-text-splitters langchain-community langgraph\n",
        "!pip install -qU \"langchain[openai]\" # select chat model OpenAI\n",
        "!pip install -U langchain langchain-core langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-openai # select embeddings model OpenAI\n",
        "!pip install -qU langchain-community # select vector store FAISS\n",
        "!pip install jq\n",
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "id": "EIJKBWd_fQrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU \"langchain-chroma>=0.1.2\""
      ],
      "metadata": {
        "id": "SDC_RMPazJ6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_chroma import Chroma"
      ],
      "metadata": {
        "id": "kv6jpEDCzNB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "import json\n",
        "import faiss\n",
        "import re\n",
        "import getpass\n",
        "import os"
      ],
      "metadata": {
        "id": "Qy3MpCwZFR8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain_community.document_loaders import JSONLoader\n",
        "from langchain_text_splitters import HTMLSectionSplitter\n",
        "from langchain_core.documents import Document\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.output_parsers.openai_tools import JsonOutputToolsParser\n",
        "from langchain_community.chat_models import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from langchain.chains import create_extraction_chain\n",
        "from langchain.chains import create_extraction_chain_pydantic\n",
        "from langchain import hub\n",
        "from langchain_core.pydantic_v1 import BaseModel\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "from pydantic import BaseModel\n",
        "from typing import Optional, List\n"
      ],
      "metadata": {
        "id": "ClKCfdTSGZgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
        "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")"
      ],
      "metadata": {
        "id": "YS1gRamHfY1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ec8f00f-990f-424d-a81a-3cb9d23a9e3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter API key for OpenAI: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd  /content/drive/MyDrive/ECE1508_Project/Codes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKEpu0O6fgMx",
        "outputId": "2690e589-45a4-4591-9013-42875135ce79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/ECE1508_Project/Codes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helpers"
      ],
      "metadata": {
        "id": "s5eNbOcwD6o9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Test Doc"
      ],
      "metadata": {
        "id": "MuhooPOlrhK1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def metadata_fuc(example: dict, _: dict) -> dict:\n",
        "    return {\n",
        "        \"question_text\": example.get(\"question_text\"),\n",
        "        \"Title\": example.get(\"title\", \"Untitled\"),\n",
        "        \"gold_answer\": example.get(\"gold_answer\", \"\")\n",
        "    }"
      ],
      "metadata": {
        "id": "2fF2ggfE0Vue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_documents(file_path):\n",
        "  loader=JSONLoader(\n",
        "    file_path=file_path,\n",
        "    jq_schema=\".[]\",\n",
        "    content_key=\"document_text\",\n",
        "    metadata_func=metadata_fuc\n",
        "  )\n",
        "  documents=loader.load()\n",
        "\n",
        "  return documents"
      ],
      "metadata": {
        "id": "AKoIS4oc9LFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vector Store Helpers"
      ],
      "metadata": {
        "id": "zIcK9WKO48gk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_faiss_vec_store(elemnts_to_emb, folder_name):\n",
        "  vectorstore=FAISS.from_documents(elemnts_to_emb,embedding=OpenAIEmbeddings())\n",
        "  vectorstore.save_local(folder_name)\n",
        "  return vectorstore"
      ],
      "metadata": {
        "id": "VkWWcJwu4_mh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retriver Helpers"
      ],
      "metadata": {
        "id": "o7-Cx6fN6Off"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# retrieve the top K contents\n",
        "def retrieve_section(in_retriever,query,top_k):\n",
        "  results=in_retriever.get_relevant_documents(query)\n",
        "  if not results:\n",
        "    return None\n",
        "  top_5_match=results[:top_k]\n",
        "  # for i in range(5):\n",
        "  #   print(f\"No.{i+1} chunk: {top_5_match[i]}\")\n",
        "\n",
        "  return top_5_match\n"
      ],
      "metadata": {
        "id": "b5gC4gab6QCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Run retriver for the input query\n",
        "def get_retrieve_section(in_retriever,in_query,top_k):\n",
        "  relevant_sections=retrieve_section(in_retriever,in_query,top_k)\n",
        "  return relevant_sections"
      ],
      "metadata": {
        "id": "z-N6Hacn6SPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Level 1 Helpers"
      ],
      "metadata": {
        "id": "XrOFCx9fqrn4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Document Chunking**"
      ],
      "metadata": {
        "id": "kiNotLnAurs6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_element_chunk(split_header_list,doc_to_chunk):\n",
        "\n",
        "  #Wrap the the original HTML content in a temporary Document object\n",
        "  html_doc = Document(page_content=doc_to_chunk.page_content, metadata=doc_to_chunk.metadata)\n",
        "\n",
        "  #Only split the HTML part\n",
        "  html_splitter = HTMLSectionSplitter(headers_to_split_on=split_header_list)\n",
        "  elements_chunked = html_splitter.split_documents([html_doc])\n",
        "  return elements_chunked\n"
      ],
      "metadata": {
        "id": "927QAUBXtetL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## L2 Proposition Helpers\n",
        "\n",
        "Reference:https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb\n",
        "\n",
        "\n",
        " The proposition promt come from: https://smith.langchain.com/hub/wfh/proposal-indexing?organizationId=50995362-9ea0-4378-ad97-b4edae2f9f22\n"
      ],
      "metadata": {
        "id": "iZwwrmv1hFya"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup proposition models"
      ],
      "metadata": {
        "id": "-71C2WjY2dsk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Sentences(BaseModel):\n",
        "    sentences: List[str]\n",
        "\n",
        "parser = PydanticOutputParser(pydantic_object=Sentences)\n",
        "\n",
        "prompt = PromptTemplate.from_template(\"\"\"\n",
        "Decompose the Passage below into clear and simple propositions, ensuring they are interpretable out of\n",
        "context.\n",
        "Note:  **If you think the input cannot be break down into proposition, provide an empty return list, don't provide an error output**\n",
        "1. Split compound sentence into simple sentences. Maintain the original phrasing from the input\n",
        "whenever possible.\n",
        "2. For any named entity that is accompanied by additional descriptive information, separate this\n",
        "information into its own distinct proposition.\n",
        "3. Decontextualize the proposition by adding necessary modifier to nouns or entire sentences\n",
        "and replacing pronouns (e.g., \"it\", \"he\", \"she\", \"they\", \"this\", \"that\") with the full name of the\n",
        "entities they refer to.\n",
        "4. Present the results as a list of strings, formatted in JSON.\n",
        "\n",
        "\n",
        "Passage:\n",
        "{input}\n",
        "\n",
        "Example:\n",
        "\n",
        "Input: Title: ¯Eostre. Section: Theories and interpretations, Connection to Easter Hares. Content:\n",
        "The earliest evidence for the Easter Hare (Osterhase) was recorded in south-west Germany in\n",
        "1678 by the professor of medicine Georg Franck von Franckenau, but it remained unknown in\n",
        "other parts of Germany until the 18th century. Scholar Richard Sermon writes that \"hares were\n",
        "frequently seen in gardens in spring, and thus may have served as a convenient explanation for the\n",
        "origin of the colored eggs hidden there for children. Alternatively, there is a European tradition\n",
        "that hares laid eggs, since a hare’s scratch or form and a lapwing’s nest look very similar, and\n",
        "both occur on grassland and are first seen in the spring. In the nineteenth century the influence\n",
        "of Easter cards, toys, and books was to make the Easter Hare/Rabbit popular throughout Europe.\n",
        "German immigrants then exported the custom to Britain and America where it evolved into the\n",
        "Easter Bunny.\"\n",
        "Output: [ \"The earliest evidence for the Easter Hare was recorded in south-west Germany in\n",
        "1678 by Georg Franck von Franckenau.\", \"Georg Franck von Franckenau was a professor of\n",
        "medicine.\", \"The evidence for the Easter Hare remained unknown in other parts of Germany until\n",
        "the 18th century.\", \"Richard Sermon was a scholar.\", \"Richard Sermon writes a hypothesis about\n",
        "the possible explanation for the connection between hares and the tradition during Easter\", \"Hares\n",
        "were frequently seen in gardens in spring.\", \"Hares may have served as a convenient explanation\n",
        "for the origin of the colored eggs hidden in gardens for children.\", \"There is a European tradition\n",
        "that hares laid eggs.\", \"A hare’s scratch or form and a lapwing’s nest look very similar.\", \"Both\n",
        "hares and lapwing’s nests occur on grassland and are first seen in the spring.\", \"In the nineteenth\n",
        "century the influence of Easter cards, toys, and books was to make the Easter Hare/Rabbit popular\n",
        "throughout Europe.\", \"German immigrants exported the custom of the Easter Hare/Rabbit to\n",
        "Britain and America.\", \"The custom of the Easter Hare/Rabbit evolved into the Easter Bunny in\n",
        "Britain and America.\"]\n",
        "\n",
        "Return the result in the following JSON format :\n",
        "{{\"sentences\": [\"sentence1\", \"sentence2\", ...]}}\n",
        "\n",
        "Important: If you cannot extract any propositions from the input, return only an empty list like this: []. Do not return any explanation, message, or error. You must always return a valid JSON list of strings — even if it’s empty\n",
        "\n",
        "\"\"\")\n",
        "\n"
      ],
      "metadata": {
        "id": "nQoBmAYvlqqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Put propositioned result into a new doc\n",
        "def get_new_prop_doc(relevant_sections, prop_results):\n",
        "  proposition_docs = []\n",
        "  for original_doc, result in zip(relevant_sections, prop_results):\n",
        "      for sentence in result.sentences:\n",
        "        proposition_docs.append(Document(\n",
        "              page_content=sentence,\n",
        "              metadata=original_doc.metadata  # retain the original metadata\n",
        "          ))\n",
        "  return proposition_docs"
      ],
      "metadata": {
        "id": "JLfPGZO33R_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main RAG end to end: Generation\n"
      ],
      "metadata": {
        "id": "E8JCYikZ7Vi9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Vector stpre folder path\n",
        "#Recheck pwd\n",
        "!ls\n",
        "L1_vector_folder = 'L1_vector_test'\n",
        "L2_vector_folder = 'L2_vector_prop'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nI2GBr_cECn7",
        "outputId": "3ff3ab74-def3-4b28-917d-89184166796f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Evaluation.ipynb\t        L1_vector_test_2\t     rag_sw_ver2.ipynb\n",
            " gold_test_file_30.json         L2_vector_prop\t\t     run_results_proposition.json\n",
            "'L1_Process_Chunk&Save.ipynb'   Proposition_Complete.ipynb   test_single_doc.json\n",
            " L1_vector\t\t        Proposition_Light.ipynb\n",
            " L1_vector_test\t\t        Proposition_Sample.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path=\"gold_test_file_30.json\"\n",
        "test_documents = load_documents(file_path)"
      ],
      "metadata": {
        "id": "HwvSRP-o8zCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Load L1 local vectorstore"
      ],
      "metadata": {
        "id": "2moeWDVrztam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = OpenAIEmbeddings()"
      ],
      "metadata": {
        "id": "vvaAm15Xz0xJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "L1_vectorstore = Chroma(\n",
        "    persist_directory=L1_vector_folder,\n",
        "    embedding_function=embeddings\n",
        ")"
      ],
      "metadata": {
        "id": "g7q8aDgEzyVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Verify local load result\n",
        "total_docs = L1_vectorstore._collection.count()\n",
        "if total_docs > 0:\n",
        "    print(f\"Vectorstore contains {total_docs} documents\")\n",
        "else:\n",
        "    print(\"Vectorstore is empty\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snuinIXx2GBC",
        "outputId": "430ba79e-445c-4bb5-a5a3-0eef9682b744"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vectorstore contains 1496 documents\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: When user input a query, start L1 Retrival + L2 Chunking"
      ],
      "metadata": {
        "id": "IOFPdoFqAGuM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Collect All questions from All documents\n",
        "test_questions = [\n",
        "    eachDoc.metadata[\"question_text\"]\n",
        "    for eachDoc in test_documents\n",
        "    if \"question_text\" in eachDoc.metadata\n",
        "]\n",
        "\n",
        "print(test_questions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Px1iWBk1HyM4",
        "outputId": "1a28a5d5-d98a-4b65-834c-4cf2f901ed5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['what episode does olivia die in the walking dead', 'actor playing krishna in mahabharat on star plus', 'is the game show the chase still on tv', 'when was figure skating introduced to the olympics', 'setting of the ones who walk away from omelas', 'who does the voice of angela on family guy', 'how many episodes of gossip girl is there', 'who plays amy on the secret life of an american teenager', 'where is the roman forum located in rome', 'where is archangel raphael mentioned in the bible', 'who plays chuck on the tv show chuck', \"who won the women's world cup championship in 2017\", 'what is the southern most point in canada', 'how many episodes will prison break season 5 have', 'what does c class stand for in mercedes benz', 'how many years did it take to build the colosseum in rome', 'who is the present governor of puerto rico', 'when does episode 131 come out for dragon ball super', \"what's the hottest natural pepper in the world\", 'list of submissions to the 87th academy awards for best foreign language film', 'who recorded the song do you love me', \"was dwight the father of angela's baby\", 'the standard deviation of all possible values is called the', 'how many episodes are in season 7 of heartland', 'what does ark mean in ark survival evolved', 'which olsen twin was in full house more', 'when do cells in the kidneys secrete renin quizlet', 'where does south african airways fly in the us', 'where will the 2018 world cup final be held', \"who played denny duquette in grey's anatomy\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Set L1 retriever\n",
        "L1_retriever=L1_vectorstore.as_retriever(search_type=\"mmr\",search_kwargs={\"k\":5})"
      ],
      "metadata": {
        "id": "KsrGLTb3AW44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#L2 Model setup\n",
        "obj = hub.pull(\"wfh/proposal-indexing\")\n",
        "llm = ChatOpenAI(model='gpt-3.5-turbo', openai_api_key =  os.environ[\"OPENAI_API_KEY\"])\n",
        "#gpt-4-turbo\n",
        "#gpt-3.5-turbo\n",
        "L2_runnable = prompt | llm | parser"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dr7zjyszBMPR",
        "outputId": "d087edbd-3a86-4ef0-f897-6670baa0fecb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langsmith/client.py:278: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# call remaining processing part + print result\n",
        "def get_L2_retrival_result(in_query,top_k,result_top_k):\n",
        "  L2_result = L1_Retrival_L2_Complete(in_query,top_k,result_top_k)\n",
        "  print(f\"--L2 Retrieved {len(L2_result)} relavant sections\")\n",
        "  #Debug\n",
        "  print(L2_result)\n",
        "  return L2_result"
      ],
      "metadata": {
        "id": "9WzxcGD3Aq9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The main funciton that complete L1 retrival + L2 chunking&retrieval\n",
        "def L1_Retrival_L2_Complete(in_query,top_k,result_top_k):\n",
        "\n",
        "  #Step 1: Get L1 relavant section based on the input question\n",
        "  L1_relevant_sections= retrieve_section(L1_retriever,in_query,top_k)\n",
        "  print(f\"--- L1 Retrieved {len(L1_relevant_sections)} relavant sections\")\n",
        "  L1_relevant_sections_texts = [doc.page_content for doc in L1_relevant_sections]\n",
        "\n",
        "  #For Debug\n",
        "  #print(L1_relevant_sections_texts)\n",
        "\n",
        "  # Step 2: Start L2 based on L1 result\n",
        "  #Run Proposition Model\n",
        "  print(f\"Running Proposition...\")\n",
        "  L2_prop_results = [L2_runnable.invoke({\"input\": text}) for text in L1_relevant_sections_texts]\n",
        "  if not L2_prop_results:\n",
        "    return 'No Found'\n",
        "  proposition_docs = get_new_prop_doc(L1_relevant_sections, L2_prop_results)\n",
        "  print(f\"{len(proposition_docs)} proposition docs to be embeded\")\n",
        "\n",
        "  if len(proposition_docs)>1:\n",
        "  #Save L2 result to vector database\n",
        "    L2_vectorstore=create_faiss_vec_store(proposition_docs,L2_vector_folder)\n",
        "    L2_retriever=L2_vectorstore.as_retriever(search_type=\"similarity\",search_kwargs={\"k\":result_top_k})\n",
        "\n",
        "    #L2 Retrieval\n",
        "    L2_relevant_sections=retrieve_section(L2_retriever,in_query,result_top_k)\n",
        "    L2_relevant_sec_text = [eachL2Doc.page_content for eachL2Doc in L2_relevant_sections]\n",
        "  else:\n",
        "    return 'No Found'\n",
        "\n",
        "  return L2_relevant_sec_text\n"
      ],
      "metadata": {
        "id": "CSJuyBBvArtm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_API_response(client,sys_prompt,user_prompt,temp,topp):\n",
        "  completion=client.chat.completions.create(\n",
        "      model=\"gpt-4o\",\n",
        "      temperature=temp,\n",
        "      top_p=topp,\n",
        "      messages=[\n",
        "          {\"role\":\"system\",\"content\":sys_prompt},\n",
        "          {\"role\":\"user\",\"content\":user_prompt}\n",
        "      ],\n",
        "  )\n",
        "  response=completion.choices[0].message.content\n",
        "  return response"
      ],
      "metadata": {
        "id": "sUiYsnUA8Nkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_question_to_gold_answer_map(in_documents):\n",
        "\n",
        "    question_to_gold = {}\n",
        "\n",
        "    for doc in in_documents:\n",
        "        question = doc.metadata.get(\"question_text\", \"\").strip()\n",
        "        gold = doc.metadata.get(\"gold_answer\", {})\n",
        "        question_to_gold[question] = gold\n",
        "\n",
        "    return question_to_gold\n"
      ],
      "metadata": {
        "id": "PHRo-2p0Ab7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def RAG_generation(questions,top_k,result_top_k,question_to_gold_map,mode='test'):\n",
        "  answer_NQ_client=OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
        "  temp=1.0\n",
        "  topp=1.0\n",
        "\n",
        "  run_results = []\n",
        "\n",
        "  for i in range(len(questions)):\n",
        "    question_curr = questions[i]\n",
        "    print(f\"============={i+1} Question:{question_curr}=============\")\n",
        "\n",
        "    relevant_content = get_L2_retrival_result(question_curr,top_k,result_top_k)\n",
        "    sys_prompt=\"\"\n",
        "    user_prompt=f\"\"\"\n",
        "    Answer the question based on the relevent contents. If you don't know the answer, say 'I don't have the answer'\n",
        "    Question: {question_curr}\n",
        "    Relevant contents:{relevant_content}\n",
        "    \"\"\"\n",
        "    response=get_API_response(answer_NQ_client,sys_prompt,user_prompt,temp,topp)\n",
        "    print('-------Final Answer:-------------')\n",
        "    print(f\"Question: {question_curr}\\n Response: {response}\")\n",
        "\n",
        "    #save to output file\n",
        "    gold_answer = question_to_gold_map.get(question_curr.strip(), \"\")\n",
        "    run_results.append({\n",
        "      \"input_question\": question_curr,\n",
        "      \"retrieved_contexts\": relevant_content,\n",
        "      \"response\": response,\n",
        "      \"gold_answer\": gold_answer\n",
        "    })\n",
        "\n",
        "    if mode == 'test':\n",
        "      break\n",
        "    else:\n",
        "      continue\n",
        "\n",
        "  return run_results\n"
      ],
      "metadata": {
        "id": "lsZHGQIh7XIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_test_output(run_results):\n",
        "  output_path = \"./evaluation/run_results_proposition_.json\"\n",
        "\n",
        "  with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "      json.dump(run_results, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "  print(f\"Saved {len(run_results)} results to {output_path}\")"
      ],
      "metadata": {
        "id": "98ft_FWKEB9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "L1_top_k =3\n",
        "result_top_k =5\n",
        "question_to_gold_map = build_question_to_gold_answer_map(test_documents)\n",
        "run_results = RAG_generation(test_questions,L1_top_k,result_top_k,question_to_gold_map,mode='test')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21DvOtdV8ZCc",
        "outputId": "dac1c773-3618-4b64-a386-8d1fbe9b4c12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=============1 Question:what episode does olivia die in the walking dead=============\n",
            "--- L1 Retrieved 3 relavant sections\n",
            "Running Proposition...\n",
            "26 proposition docs to be embeded\n",
            "--L2 Retrieved 5 relavant sections\n",
            "['Olivia is portrayed by Ann Mahoney on The Walking Dead television series.', 'Olivia is threatened with death by Negan in the episode \"Service\" when the Saviors find out that two of the guns are missing from the armory.', 'Olivia is the main caretaker of Judith Grimes in this episode.', 'Olivia is a member of the supporting cast for the seventh season.', 'Arat pulls out her gun, turns around, and shoots Olivia in the face, killing her.']\n",
            "-------Final Answer:-------------\n",
            "Question: what episode does olivia die in the walking dead\n",
            " Response: I don't have the answer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_test_output(run_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RK5xF4BXEXMW",
        "outputId": "0be331fc-0959-411f-b326-8b300e5ab0ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved 1 results to run_results_proposition.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**: When run with GPT4, the response is pretty accurate.\n",
        "when L1_top_k =3, result_top_k =5, the first example results in 12 proposition, and the cost is around $0.04, the result is accurate."
      ],
      "metadata": {
        "id": "ncDDgCU7E7ny"
      }
    }
  ]
}