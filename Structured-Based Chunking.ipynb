{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPGwXIzoJI5yXjRZD27FUi6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"G2H1qOhGZ_Db"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install openai sentence-transformers faiss-cpu beautifulsoup4\n","!pip install chromadb langchain langchain_community langchain_openai"],"metadata":{"id":"JYi_rMaTaAet"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BKhdzEIZZ98E"},"outputs":[],"source":["import requests\n","from bs4 import BeautifulSoup\n","import re\n","import json\n","import uuid\n","import logging\n","import numpy as np\n","import faiss\n","from typing import List, Dict\n","from sentence_transformers import SentenceTransformer\n","\n","class AdvancedDocumentChunker:\n","    def __init__(self, embedding_model='sentence-transformers/all-mpnet-base-v2'):\n","        \"\"\"\n","        Initialize document chunker and embedding model\n","        \"\"\"\n","        self.embedding_model = SentenceTransformer(embedding_model)\n","        self.vector_index = None\n","        self.metadata = []\n","        self.dimension = None\n","\n","    def _process_table(self, table) -> str:\n","        \"\"\"\n","        Convert table to pure text paragraph without any markdown symbols\n","        Format: \"Table [caption] shows: [header1] is [value1], [header2] is [value2]...\"\n","        \"\"\"\n","        try:\n","            # Skip navigation tables\n","            if table.get('class') and any(c in ['navbox', 'sidebar', 'infobox'] for c in table.get('class')):\n","                return ''\n","\n","            # 1. Extract caption\n","            caption = table.find('caption')\n","            caption_text = caption.get_text(strip=True) if caption else \"the data\"\n","\n","            # 2. Extract headers (th or first row's td)\n","            headers = []\n","            header_row = table.find('tr')\n","            if header_row:\n","                headers = [th.get_text(\" \", strip=True)\n","                        for th in header_row.find_all(['th', 'td'])]\n","\n","            # 3. Process all rows\n","            paragraphs = []\n","            for i, tr in enumerate(table.find_all('tr')[1 if headers else 0:]):\n","                cells = [td.get_text(\" \", strip=True) for td in tr.find_all(['td', 'th'])]\n","                if not cells or not any(cell.strip() for cell in cells):\n","                    continue\n","\n","                # Build sentence for each row\n","                if headers:\n","                    # With headers: \"ColumnA is value1, ColumnB is value2\"\n","                    row_desc = []\n","                    for j, val in enumerate(cells):\n","                        header = headers[j] if j < len(headers) else f\"Column {j+1}\"\n","                        if val.strip():\n","                            row_desc.append(f\"{header} is {val}\")\n","                else:\n","                    # Without headers: \"Row 1: value1, value2...\"\n","                    row_desc = [f\"Row {i+1}: {', '.join(cells)}\"]\n","\n","                paragraphs.append(\", \".join(row_desc) + \".\")\n","\n","            if not paragraphs:\n","                return \"\"\n","\n","            # 4. Combine into final paragraph\n","            return f\"Table '{caption_text}' shows: \" + \" \".join(paragraphs)\n","\n","        except Exception as e:\n","            logging.error(f\"Table processing error: {str(e)}\")\n","            return \"\"\n","\n","    def clean_text(self, text: str) -> str:\n","        \"\"\"Clean text content\"\"\"\n","        try:\n","            soup = BeautifulSoup(text, 'html.parser')\n","            text = soup.get_text(separator=' ', strip=True)\n","        except Exception as e:\n","            logging.warning(f\"HTML parsing warning: {e}\")\n","\n","        text = re.sub(r'\\s+', ' ', text).strip()\n","\n","        return text\n","    def extract_sections(self, text: str) -> List[Dict]:\n","        \"\"\"\n","        Enhanced section extraction with comprehensive reference removal\n","        Properly handles H4 tags to maintain heading hierarchy\n","        \"\"\"\n","        if not isinstance(text, str):\n","            text = str(text)\n","\n","        soup = BeautifulSoup(text, 'html.parser')\n","\n","        # Enhanced reference removal - handle all known reference forms\n","        reference_selectors = [\n","            # Wikipedia standard references\n","            'sup.reference', 'span.mw-cite-backlink',\n","            # General citation markers\n","            'span.citation', 'span.footnote', 'div.footnotes',\n","            # Reference blocks\n","            'ol.references', 'div.reflist', 'div.refbegin',\n","            # Hidden content\n","            'div.noprint', 'span.mw-editsection',\n","            # Citation links\n","            'a[href^=\"#cite\"]', 'a[href*=\"wikisource\"]',\n","            # Modern HTML5 notes\n","            '[role=\"doc-noteref\"]', '[role=\"doc-endnotes\"]'\n","        ]\n","\n","        for selector in reference_selectors:\n","            for ref in soup.select(selector):\n","                ref.decompose()\n","\n","        # Additionally clean [1][2] text references\n","        for element in soup.find_all(string=True):\n","            if isinstance(element, str):\n","                cleaned = re.sub(r'\\[\\d+\\]', '', element)\n","                if cleaned != element:\n","                    element.replace_with(cleaned)\n","\n","        sections = []\n","        is_wikipedia = 'wikipedia' in text.lower() or bool(soup.select_one('.mw-parser-output'))\n","\n","        if is_wikipedia:\n","            main_content = soup.select_one('.mw-parser-output')\n","            if main_content:\n","                current_hierarchy = []\n","                current_content = []\n","                current_h4 = None  # Track current H4 title\n","\n","                for elem in main_content.children:\n","                    if elem.name in ['h2', 'h3']:\n","                        # Save current section\n","                        if current_hierarchy and current_content:\n","                            # Include current H4 in the title if it exists\n","                            title = ' > '.join(current_hierarchy)\n","                            if current_h4:\n","                                title = f\"{title} > {current_h4}\"\n","                                current_h4 = None  # Reset H4 tracking\n","\n","                            sections.append({\n","                                'document_id': str(uuid.uuid4()),\n","                                'section': {\n","                                    'title': title,\n","                                    'content': self.clean_text(' '.join(current_content))\n","                                }\n","                            })\n","                            current_content = []\n","\n","                        # Update hierarchy\n","                        level = int(elem.name[1])\n","                        title = elem.get_text(strip=True)\n","                        current_hierarchy = current_hierarchy[:level-2] + [title]\n","\n","                    elif elem.name == 'h4':\n","                        # If we already have content, save the current subsection before starting a new H4\n","                        if current_content and current_hierarchy:\n","                            title = ' > '.join(current_hierarchy)\n","                            if current_h4:\n","                                title = f\"{title} > {current_h4}\"\n","\n","                            sections.append({\n","                                'document_id': str(uuid.uuid4()),\n","                                'section': {\n","                                    'title': title,\n","                                    'content': self.clean_text(' '.join(current_content))\n","                                }\n","                            })\n","                            current_content = []\n","\n","                        # Set new H4 as current\n","                        current_h4 = elem.get_text(strip=True)\n","\n","                    elif elem.name in ['p', 'ul', 'ol', 'table']:\n","                        # Process content elements\n","                        if elem.name == 'p':\n","                            text = elem.get_text(strip=True)\n","                            if text: current_content.append(text)\n","                        elif elem.name in ['ul', 'ol']:\n","                            items = [li.get_text(strip=True) for li in elem.find_all('li')]\n","                            if items: current_content.append('; '.join(items))\n","                        elif elem.name == 'table':\n","                            if not any(c in elem.get('class', [])\n","                                    for c in ['navbox', 'infobox', 'sidebar']):\n","                                table_text = self._process_table(elem)\n","                                if table_text: current_content.append(table_text)\n","\n","                # Add the last section\n","                if current_hierarchy and current_content:\n","                    title = ' > '.join(current_hierarchy)\n","                    if current_h4:\n","                        title = f\"{title} > {current_h4}\"\n","\n","                    sections.append({\n","                        'document_id': str(uuid.uuid4()),\n","                        'section': {\n","                            'title': title,\n","                            'content': self.clean_text(' '.join(current_content))\n","                        }\n","                    })\n","\n","        # Generic processing for non-Wikipedia content (similar approach can be applied here)\n","        if not sections:\n","            current_section = None\n","            current_h4 = None\n","            for elem in soup.find_all(['h1', 'h2', 'h3', 'h4', 'p', 'ul', 'ol', 'table']):\n","                if elem.name in ['h1', 'h2', 'h3']:\n","                    if current_section and current_section['section']['content']:\n","                        # Include H4 in title if present\n","                        if current_h4:\n","                            current_section['section']['title'] += f\" > {current_h4}\"\n","                            current_h4 = None\n","\n","                        # Convert content from list to string\n","                        if isinstance(current_section['section']['content'], list):\n","                            current_section['section']['content'] = self.clean_text(' '.join(current_section['section']['content']))\n","\n","                        sections.append(current_section)\n","\n","                    current_section = {\n","                        'document_id': str(uuid.uuid4()),\n","                        'section': {\n","                            'title': elem.get_text(strip=True),\n","                            'content': []\n","                        }\n","                    }\n","                elif elem.name == 'h4':\n","                    # Save current subsection if it has content\n","                    if current_section and current_section['section']['content']:\n","                        title = current_section['section']['title']\n","                        if current_h4:\n","                            title += f\" > {current_h4}\"\n","\n","                        sections.append({\n","                            'document_id': str(uuid.uuid4()),\n","                            'section': {\n","                                'title': title,\n","                                'content': self.clean_text(' '.join(current_section['section']['content']))\n","                            }\n","                        })\n","\n","                        # Reset content but keep the same parent section\n","                        current_section = {\n","                            'document_id': str(uuid.uuid4()),\n","                            'section': {\n","                                'title': title.split(' > ')[0],  # Keep original parent title\n","                                'content': []\n","                            }\n","                        }\n","\n","                    # Update H4 title\n","                    current_h4 = elem.get_text(strip=True)\n","\n","                elif current_section:\n","                    if elem.name == 'p':\n","                        text = elem.get_text(strip=True)\n","                        if text: current_section['section']['content'].append(text)\n","                    elif elem.name in ['ul', 'ol']:\n","                        items = [li.get_text(strip=True) for li in elem.find_all('li')]\n","                        if items: current_section['section']['content'].append('; '.join(items))\n","                    elif elem.name == 'table':\n","                        table_text = self._process_table(elem)\n","                        if table_text: current_section['section']['content'].append(table_text)\n","\n","            if current_section and current_section['section']['content']:\n","                # Include H4 in final section if present\n","                if current_h4:\n","                    current_section['section']['title'] += f\" > {current_h4}\"\n","\n","                # Convert content from list to string if needed\n","                if isinstance(current_section['section']['content'], list):\n","                    current_section['section']['content'] = self.clean_text(' '.join(current_section['section']['content']))\n","\n","                sections.append(current_section)\n","\n","        # Fallback: if no sections were found, return the entire document\n","        if not sections:\n","            full_text = ' '.join(p.get_text(strip=True) for p in soup.find_all(['p', 'li']))\n","            if full_text:\n","                sections.append({\n","                    'document_id': str(uuid.uuid4()),\n","                    'section': {\n","                        'title': 'Full Document',\n","                        'content': self.clean_text(full_text)\n","                    }\n","                })\n","\n","        logging.info(f\"Extracted {len(sections)} sections\")\n","        return sections\n","\n","\n","    def chunk_and_index(self, documents, max_documents=100):\n","        \"\"\"Improved version with better debugging\"\"\"\n","        documents = documents[:max_documents]\n","        all_sections = []\n","\n","        for doc_index, doc in enumerate(documents):\n","            try:\n","                text_fields = ['document_text', 'html_content', 'text', 'content']\n","                text = next((doc[field] for field in text_fields if field in doc and doc[field]), str(doc))\n","\n","                sections = self.extract_sections(text)\n","                print(f\"\\nDocument {doc_index} - Extracted {len(sections)} sections:\")\n","\n","                for i, section in enumerate(sections, 1):\n","                    print(f\"  Section {i}: {section['section']['title']} \"\n","                          f\"(Length: {len(section['section']['content'])} chars)\")\n","                    all_sections.append({\n","                        'document_id': f'Doc_{doc_index}_Sec_{i}',\n","                        'section': section['section']\n","                    })\n","\n","            except Exception as e:\n","                print(f\"Error processing document {doc_index}: {str(e)}\")\n","\n","        if not all_sections:\n","            print(\"Warning: No sections extracted - using fallback chunking\")\n","            return self.fallback_chunking(documents)\n","\n","        # Generate embeddings with title emphasis\n","        section_texts = [\n","            f\"TITLE: {s['section']['title']}\\nCONTENT: {s['section']['content']}\"\n","            for s in all_sections\n","        ]\n","\n","        print(f\"\\nGenerating embeddings for {len(section_texts)} sections...\")\n","        embeddings = self.embedding_model.encode(section_texts, convert_to_numpy=True)\n","\n","        # Create and populate index\n","        self.dimension = embeddings.shape[1]\n","        print(f\"Embedding dimension: {self.dimension}\")\n","\n","        self.vector_index = faiss.IndexFlatIP(self.dimension)  # Using Inner Product for cosine similarity\n","        faiss.normalize_L2(embeddings)  # Normalize for cosine similarity\n","        self.vector_index.add(embeddings)\n","        self.metadata = all_sections\n","\n","        print(f\"Index built with {self.vector_index.ntotal} vectors\")\n","        return all_sections\n","\n","    def retrieve_sections(self, query: str, top_k: int = 3):\n","        \"\"\"Improved retrieval with better scoring\"\"\"\n","        if self.vector_index is None:\n","            return []\n","\n","        # Encode and normalize query\n","        query_embedding = self.embedding_model.encode([query], convert_to_numpy=True)[0]\n","        query_embedding = query_embedding / np.linalg.norm(query_embedding)\n","\n","        # Search with cosine similarity\n","        D, I = self.vector_index.search(np.array([query_embedding]).astype('float32'), top_k)\n","\n","        results = []\n","        for i, idx in enumerate(I[0]):\n","            meta = self.metadata[idx]\n","            similarity = (1 + D[0][i]) / 2  # Convert from [-1,1] to [0,1] range\n","            results.append({\n","                'document': meta['document_id'],\n","                'title': meta['section']['title'],\n","                'content': meta['section']['content'],\n","                'similarity': f\"{similarity:.2%}\"\n","            })\n","\n","        return sorted(results, key=lambda x: float(x['similarity'].strip('%')), reverse=True)[:top_k]\n","\n","    def save_to_vectorstore(self, persist_directory, embedding_model=None):\n","        \"\"\"\n","        Save the processed sections to a Chroma vector store for later retrieval\n","\n","        Args:\n","            persist_directory: Directory to save the vector store\n","            embedding_model: LangChain embedding model (default: OpenAIEmbeddings)\n","        \"\"\"\n","        try:\n","            from langchain_community.vectorstores import Chroma\n","            from langchain.embeddings import OpenAIEmbeddings\n","        except ImportError:\n","            print(\"Need to install langchain packages: pip install langchain langchain-community\")\n","            return None\n","\n","        if embedding_model is None:\n","            try:\n","                embedding_model = OpenAIEmbeddings()\n","            except Exception as e:\n","                print(f\"Error creating OpenAIEmbeddings: {e}\")\n","                print(\"Using a dummy embedding function instead\")\n","                class DummyEmbeddings:\n","                    def embed_documents(self, texts):\n","                        return [self.embed_query(text) for text in texts]\n","                    def embed_query(self, text):\n","                        return [0.0] * self.dimension\n","                embedding_model = DummyEmbeddings()\n","                embedding_model.dimension = self.dimension\n","\n","        # Convert sections to LangChain documents\n","        from langchain.schema import Document\n","        documents = []\n","        for section in self.metadata:\n","            documents.append(\n","                Document(\n","                    page_content=section['section']['content'],\n","                    metadata={\n","                        \"title\": section['section']['title'],\n","                        \"source\": section['document_id']\n","                    }\n","                )\n","            )\n","\n","        # Create and persist the vector store\n","        vectorstore = Chroma.from_documents(\n","            documents=documents,\n","            embedding=embedding_model,\n","            persist_directory=persist_directory\n","        )\n","        vectorstore.persist()\n","\n","        print(f\"Saved {len(documents)} sections to vector store at {persist_directory}\")\n","        return vectorstore\n","\n","# Example usage\n","def main():\n","    # Wikipedia URL for The Avengers film\n","    url = 'https://en.wikipedia.org/wiki/The_Avengers_(2012_film)'\n","\n","    # Fetch HTML content\n","    response = requests.get(url)\n","\n","    # Create a test document dictionary\n","    test_document = [{\n","        'document_text': response.text\n","    }]\n","\n","    # Initialize chunker\n","    chunker = AdvancedDocumentChunker()\n","\n","    # Process the URL and get sections\n","    sections = chunker.extract_sections(response.text)\n","\n","    # Print out the sections for verification\n","    print(f\"Total Sections: {len(sections)}\\n\")\n","    for i, section in enumerate(sections, 1):\n","        print(f\"Section {i} - Title: {section['section']['title']}\")\n","        print(f\"Content Preview: {section['section']['content']}...\\n\")\n","\n","    # Chunk and index\n","    print(\"\\n===== Chunk and Index =====\")\n","    indexed_sections = chunker.chunk_and_index(test_document)\n","\n","    print(\"\\n===== Saving to Vector Store =====\")\n","    vector_store_path = \"L1_vector_final\"\n","    chunker.save_to_vectorstore(vector_store_path)\n","\n","    # Test retrieval\n","    test_queries = [\n","        \"Who are the main actors?\",\n","        \"When was the movie released?\",\n","        \"What is the plot of the movie?\"\n","    ]\n","\n","    print(\"\\n===== Query Retrieval =====\")\n","    for query in test_queries:\n","        print(f\"\\nQuery: {query}\")\n","        results = chunker.retrieve_sections(query, top_k=2)\n","        for i, result in enumerate(results, 1):\n","            print(f\"\\nResult {i} (Similarity: {result['similarity']}):\")\n","            print(f\"Title: {result['title']}\")\n","            print(f\"Content: {result['content'][:100]}...\")\n","\n","if __name__ == '__main__':\n","    main()"]}]}